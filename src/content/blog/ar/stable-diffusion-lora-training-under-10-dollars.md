---
title: "كيفية تدريب نماذج Stable Diffusion LoRA بأقل من 10 دولارات"
description: "دليل عملي خطوة بخطوة لتدريب نماذج LoRA مخصصة لـ Stable Diffusion باستخدام وحدات GPU مستأجرة. شرح شامل لاختيار وحدة المعالجة الرسومية المناسبة، إعداد مجموعة البيانات، ضبط إعدادات التدريب، وتحسين التكلفة."
excerpt: "دليل تطبيقي لتدريب نماذج LoRA عالية الجودة باستخدام استئجار وحدات GPU. يتناول اختيار المزود، الضبط الصحيح، وتقنيات تقليل التكلفة إلى أقل من 10 دولارات."
pubDate: 2026-02-11
updatedDate: 2026-02-11
locale: "ar"
category: "tutorials"
featured: false
draft: false
author: "GPUFlow Team"
heroImage: "../_images/stable-diffusion-lora-training-guide.jpg"
heroImageAlt: "بطاقة رسومية من NVIDIA مثبتة داخل خادم مع مراوح تبريد وإضاءة LED ظاهرة"
faq:
  - question: "هل يمكنني تدريب نماذج LoRA باستخدام بطاقتي الرسومية الخاصة بدلاً من الاستئجار؟"
    answer: "نعم، بشرط أن تمتلك بطاقة NVIDIA بسعة لا تقل عن 12 جيجابايت VRAM مثل RTX 3060 أو أفضل. ومع ذلك، فإن تكلفة الكهرباء، واستهلاك العتاد، ومدة التدريب الأطول على الأجهزة الاستهلاكية تجعل الاستئجار غالبًا خيارًا أكثر جدوى للمشاريع غير الدائمة."
  - question: "كم يستغرق تدريب نموذج LoRA عادةً؟"
    answer: "تكتمل معظم جلسات تدريب LoRA خلال ساعة إلى ثلاث ساعات عند استخدام RTX 4090 أو RTX 3090. تعتمد المدة الدقيقة على حجم مجموعة البيانات، وعدد العصور التدريبية، وإعدادات حجم الدفعة."
  - question: "ما هو الحد الأدنى لعدد الصور المطلوبة لتدريب LoRA؟"
    answer: "يمكن الحصول على نتائج معقولة باستخدام ما بين 15 إلى 20 صورة. ومع ذلك، فإن المجموعات التي تحتوي على 30 إلى 100 صورة موصوفة بدقة تعطي نتائج أفضل بكثير. جودة الصور ودقة الوصف النصي أهم من العدد المجرد."
  - question: "أي مزود لتأجير وحدات GPU يقدم أفضل قيمة لتدريب LoRA؟"
    answer: "يوفر Vast.ai عادةً أقل سعر للساعة لبطاقات RTX 4090. تقدم GPUFlow أسعارًا تنافسية مع دعم الدفع بالعملات الرقمية ودون متطلبات تحقق من الهوية. أما RunPod فيوفر واجهة أكثر سهولة للمستخدمين الجدد في مجال تأجير وحدات GPU."
  - question: "هل من الأكثر جدوى اقتصاديًا تدريب عدة نماذج LoRA في جلسة واحدة؟"
    answer: "نعم. تدريب عدة نماذج ضمن جلسة ممتدة يقلل من وقت الإعداد المتكرر ويحد من تكاليف التشغيل الخامل. تدريب ثلاثة إلى خمسة نماذج خلال جلسة مدتها أربع ساعات غالبًا ما يكلف أقل من نصف تكلفة تدريبها كلٌ على حدة."
---

# كيفية تدريب نماذج Stable Diffusion LoRA بأقل من 10 دولارات

أصبح تدريب نماذج LoRA المخصصة لـ Stable Diffusion أحد أكثر الأساليب العملية لإنتاج صور مولدة بالذكاء الاصطناعي وفق متطلبات محددة. سواء كنت تسعى إلى إعادة إنتاج أسلوب فني معين، أو توليد شخصية بملامح متسقة، أو تحسين النموذج ليتعامل مع تصوير المنتجات، فإن تدريب LoRA يتيح تحقيق هذه الأهداف دون التكلفة الحسابية الباهظة المرتبطة بالتعديل الكامل للنموذج.

هناك افتراض شائع بأن هذه العملية تتطلب إما أجهزة محلية مرتفعة التكلفة أو ميزانية حوسبة سحابية كبيرة. في الواقع، كلا الأمرين غير دقيق. مع أسعار تأجير وحدات GPU الحالية وإعدادات تدريب محسّنة، يمكن تدريب نموذج LoRA بجودة إنتاجية بأقل من عشرة دولارات، وغالبًا بأقل من ذلك.

يستعرض هذا الدليل العملية كاملة: اختيار العتاد المناسب، إعداد مجموعة البيانات، ضبط معايير التدريب، تنفيذ جلسة التدريب، ثم التحقق من النتائج. سأكون دقيقًا فيما يتعلق بالتكاليف في كل مرحلة، لأن الوعود العامة حول “الذكاء الاصطناعي منخفض التكلفة” لا تساعد من يخطط لمشروع بميزانية فعلية.

**ما تحتاجه قبل البدء:**

- من 20 إلى 100 صورة تدريب (سنتناول معايير الاختيار لاحقًا)
- إلمام أساسي باستخدام واجهات سطر الأوامر
- محفظة عملات رقمية أو بطاقة دفع لتسديد رسوم استئجار وحدة GPU
- من ساعتين إلى أربع ساعات من وقت العمل المركز
- ميزانية تتراوح بين 5 و15 دولارًا لأول جلسة تدريب

![مركز بيانات حديث يحتوي على صفوف من خوادم GPU عالية الأداء مخصصة لأحمال تعلم الآلة](../_images/data-center-with-person.jpg)

---

## جدول المحتويات

- [فهم LoRA وأهميته](#فهم-lora-وأهميته)
- [اختيار وحدة GPU المناسبة للتدريب](#اختيار-وحدة-gpu-المناسبة-للتدريب)
- [مقارنة مزودي تأجير وحدات GPU](#مقارنة-مزودي-تأجير-وحدات-gpu)
- [إعداد مجموعة بيانات التدريب](#إعداد-مجموعة-بيانات-التدريب)
- [إعداد بيئة التدريب](#إعداد-بيئة-التدريب)
- [ضبط معايير التدريب](#ضبط-معايير-التدريب)
- [تنفيذ جلسة التدريب](#تنفيذ-جلسة-التدريب)
- [التحقق من النموذج واختباره](#التحقق-من-النموذج-واختباره)
- [استراتيجيات تحسين التكلفة](#استراتيجيات-تحسين-التكلفة)
- [المشكلات الشائعة والحلول](#المشكلات-الشائعة-والحلول)
- [الأسئلة الشائعة](#الأسئلة-الشائعة)

---

## فهم LoRA وأهميته

LoRA هو اختصار لـ Low-Rank Adaptation، وهي تقنية تتيح تعديل الشبكات العصبية الكبيرة من خلال تدريب عدد محدود من المعاملات الإضافية بدلاً من تعديل النموذج بالكامل. يحتوي نموذج Stable Diffusion الأصلي على ما يقارب مليار معامل. إن إعادة ضبط هذه المعاملات بالكامل تتطلب ذاكرة GPU كبيرة ومدة تدريب طويلة.

تتجاوز LoRA هذه المشكلة عبر تجميد أوزان النموذج الأصلي وتدريب مصفوفات صغيرة تعمل كمحوّلات تعدّل طريقة معالجة النموذج للمعلومات. يتراوح حجم ملف LoRA عادةً بين 10 و200 ميجابايت، مقارنةً بحجم يتراوح بين 2 و6 جيجابايت لنقطة تحقق كاملة من Stable Diffusion.

الآثار العملية لذلك واضحة:

**كفاءة في استهلاك الذاكرة.** يتطلب تدريب LoRA ذاكرة VRAM أقل بكثير من التدريب الكامل. يمكن لبطاقة بسعة 24 جيجابايت تدريب LoRAs لنماذج SDXL التي قد تتطلب 40 جيجابايت أو أكثر عند التعديل الكامل.

**سرعة في التدريب.** بما أنك تدرب عددًا أقل من المعاملات، فإن كل دورة تدريبية تكتمل بسرعة أكبر. ما قد يستغرق اثنتي عشرة ساعة في التدريب الكامل يمكن إنجازه في تسعين دقيقة تقريبًا باستخدام LoRA.

**إمكانية الدمج.** يمكن استخدام عدة LoRAs معًا أثناء مرحلة التوليد. يمكنك استخدام LoRA لأسلوب فني معين وأخرى لثبات الشخصية، ودمجهما بنسب مختلفة دون الحاجة لإعادة التدريب.

**سهولة التخزين والتوزيع.** الأحجام الصغيرة للملفات تجعل مشاركة LoRAs وإدارتها عملية. يمكنك الاحتفاظ بعشرات النماذج المتخصصة دون قلق بشأن المساحة التخزينية.

هذا الانخفاض في المتطلبات الحسابية هو ما يجعل التدريب بأقل من عشرة دولارات ممكنًا. فأنت تستأجر عتادًا مكلفًا لمدة ساعة إلى ثلاث ساعات فقط بدلاً من ثماني إلى أربع وعشرين ساعة.

---

## اختيار وحدة GPU المناسبة للتدريب

يتطلب اختيار وحدة GPU موازنة ثلاثة عوامل: سعة VRAM، سرعة التدريب، وتكلفة الاستئجار بالساعة. الحد الأدنى المقبول يختلف عن الخيار الأمثل.

### متطلبات VRAM

بالنسبة لتدريب LoRA على Stable Diffusion 1.5، يُعد 12 جيجابايت من VRAM الحد الأدنى العملي. يمكن تشغيل التدريب على 8 جيجابايت عبر تقليل حجم الدفعة والدقة، لكن جودة النتائج غالبًا ما تتأثر.

أما بالنسبة لتدريب LoRA على SDXL، فإن 16 جيجابايت تمثل الحد الأدنى، ويفضل 24 جيجابايت. نماذج SDXL أكبر وأكثر تطلبًا. محاولة تدريبها بذاكرة غير كافية يؤدي إلى تبديل مستمر للذاكرة (memory swapping)، ما يبطئ العملية بشكل ملحوظ وقد يؤدي إلى فشل التدريب.

### موازنة السرعة والتكلفة

البطاقات الأعلى سعرًا تُنهي التدريب بسرعة أكبر، لكن زيادة التكلفة بالساعة لا تعني دائمًا انخفاضًا متناسبًا في التكلفة الإجمالية للمشروع. لننظر إلى مقارنة تقريبية لتدريب LoRA اعتيادي على SD 1.5:

| GPU         | VRAM | مدة تدريب تقريبية | سعر الساعة | التكلفة الإجمالية |
| ----------- | ---- | ----------------- | ---------- | ----------------- |
| RTX 3090    | 24GB | 2.5 ساعة          | $0.50      | $1.25             |
| RTX 4090    | 24GB | 1.5 ساعة          | $0.70      | $1.05             |
| RTX A6000   | 48GB | 1.5 ساعة          | $0.80      | $1.20             |
| A100 (40GB) | 40GB | 1.0 ساعة          | $1.50      | $1.50             |

عادةً ما يوفر RTX 4090 أفضل كفاءة من حيث التكلفة. فهو يقترب من أداء بطاقات مراكز البيانات، لكن بسعر ساعة أقل بكثير. يظل RTX 3090 خيارًا مناسبًا عند محدودية توفر 4090، مع فارق بسيط في التكلفة النهائية.

في حالة SDXL، تصبح بطاقات مثل A100 أكثر تنافسية عند المشاريع المعقدة التي قد تستغرق أربع ساعات أو أكثر على عتاد استهلاكي.

لتحليل شامل لأسعار تأجير وحدات GPU عبر مختلف المنصات، راجع:  
[مقارنة أسعار تأجير وحدات GPU لعام 2026](/en/gpu-rental-pricing-comparison-2026/)

![بطاقة NVIDIA RTX 4090 مزودة بنظام تبريد ثلاثي المراوح تُستخدم غالبًا في تدريب نماذج الذكاء الاصطناعي](../_images/nvidia-4090.jpg)

## مقارنة مزودي تأجير وحدات GPU

هناك ثلاثة مزودين يستحقون الدراسة عند تشغيل أعباء تدريب LoRA. لكل منهم خصائص مختلفة تؤثر بحسب أسلوب الدفع الذي تفضله، ومستوى خبرتك التقنية، ومدى حساسيتك للسعر.

### Vast.ai

تعمل Vast.ai كسوق نظير إلى نظير حيث يقوم مالكو وحدات GPU بعرض عتادهم للإيجار. هذا النموذج ينتج عادةً أقل الأسعار في السوق، إذ تتوفر بطاقات RTX 4090 غالبًا ضمن نطاق يتراوح بين 0.35 و0.60 دولار في الساعة.

المقابل هو التفاوت. تختلف موثوقية المزودين من 97% إلى 99.9% حسب المضيف. كما أن التوفر يتغير بحسب الطلب. قد تحتاج إلى تجربة أكثر من مزود قبل العثور على واحد يوفر سرعة شبكة مناسبة لرفع مجموعة بياناتك.

للمستخدمين ذوي الخبرة القادرين على تقييم مؤشرات الأداء والموثوقية، توفر Vast.ai أقل تكلفة ممكنة للتدريب. من الحكمة تخصيص ثلاثين دقيقة إضافية للإعداد الأولي وتقييم المضيفين.

### RunPod

تتموضع RunPod بين الأسواق المفتوحة ومزودي الحوسبة السحابية المؤسسية. تقدم المنصة وحدات GPU من المجتمع، إضافة إلى بيئة “Secure Cloud” أكثر استقرارًا.

الأسعار أعلى قليلًا من Vast.ai، حيث يبلغ سعر RTX 4090 في بيئة Secure Cloud حوالي 0.59 دولار في الساعة. في المقابل، توفر المنصة إعدادًا أسهل، وقوالب جاهزة لأحمال الذكاء الاصطناعي الشائعة، وتوفرًا أكثر استقرارًا.

للمستخدمين الجدد في مجال تأجير وحدات GPU، أو لمن يفضلون البساطة على خفض التكلفة إلى الحد الأدنى، تمثل RunPod خيارًا متوازنًا.

### GPUFlow

تعمل GPUFlow كسوق لامركزي يعتمد على بنية بلوكتشين مع استخدام عقود ذكية لمعالجة المدفوعات. تدعم المنصة الدفع بالعملات الرقمية فقط ولا تتطلب تحققًا من الهوية.

يتراوح سعر RTX 4090 عادة بين 0.50 و0.80 دولار في الساعة. ما يميز المنصة هو خصوصية الدفع، وسرعة الإعداد (غالبًا أقل من ثلاثين ثانية للوصول إلى بيئة جاهزة)، ورسوم منصة أقل مقارنة ببعض الأسواق المنافسة.

للمستخدمين الذين يفضلون الدفع بالعملات الرقمية أو يرغبون في تجنب عمليات التحقق المطولة، توفر GPUFlow خيارًا مباشرًا وسريعًا.

### ملخص المقارنة

| المزود  | نطاق سعر RTX 4090  | زمن الإعداد | طرق الدفع           | الأنسب لـ        |
| ------- | ------------------ | ----------- | ------------------- | ---------------- |
| Vast.ai | 0.35–0.60 دولار    | 5–15 دقيقة  | بطاقة ائتمان        | أدنى تكلفة       |
| RunPod  | 0.59 دولار تقريبًا | 2–5 دقائق   | بطاقة / عملات رقمية | سهولة الاستخدام  |
| GPUFlow | 0.50–0.80 دولار    | ~30 ثانية   | عملات رقمية فقط     | الخصوصية والسرعة |

---

## إعداد مجموعة بيانات التدريب

جودة مجموعة البيانات تؤثر في النتيجة النهائية أكثر من أي عامل آخر. مجموعة مكونة من ثلاثين صورة مختارة بعناية ستنتج نتائج أفضل من مئتي صورة مجمعة بلا تدقيق.

### معايير اختيار الصور

**الاتساق.** يجب أن تمثل جميع الصور المفهوم الذي تريد للنموذج تعلمه. إذا كنت تدرب على وجه شخص معين، فيجب أن يظهر الوجه بوضوح في كل صورة. وإذا كنت تدرب على أسلوب فني، فيجب أن تعكس جميع الصور ذلك الأسلوب.

**تنوع ضمن الاتساق.** مع الحفاظ على الفكرة الأساسية، غيّر الزوايا، والإضاءة، والخلفيات، والسياقات. هذا يساعد النموذج على التعميم بدلاً من حفظ تكوينات محددة حرفيًا.

**الجودة التقنية.** استخدم صورًا واضحة جيدة التعريض. الضوضاء، والتمويه الحركي، وآثار الضغط، كلها تصبح جزءًا مما يتعلمه النموذج. إن كانت صور التدريب منخفضة الجودة، ستكون النتائج كذلك.

**الدقة.** يجب ألا تقل أبعاد الصور عن 512×512 لـ SD 1.5، و1024×1024 لـ SDXL. الدقة الأعلى تتيح الاقتصاص وإعادة التحجيم دون فقدان ملحوظ في الجودة.

### حجم مجموعة البيانات

يعتمد الحجم الأمثل على تعقيد المفهوم:

- **مفهوم بسيط (وجه واحد، أسلوب أساسي):** 20–40 صورة
- **مفهوم متوسط (شخصية بأزياء متعددة، أسلوب دقيق):** 40–80 صورة
- **مفهوم معقد (بيئات تفصيلية، تنوع كبير):** 80–150 صورة

زيادة عدد الصور تعني خطوات تدريب أكثر، وبالتالي وقتًا وتكلفة أعلى. في المحاولات الأولى، ابدأ بالحد الأدنى ضمن النطاق المقترح.

### كتابة الوصف النصي (Captioning)

كل صورة تدريب تحتاج إلى ملف نصي يصف محتواها. هذه الأوصاف تربط بين الأنماط البصرية والمفاهيم اللغوية.

الوصف الجيد يكون دقيقًا ومتسقًا.

**وصف ضعيف:**  
“امرأة”

**وصف أفضل:**  
“صورة فوتوغرافية لسارة ميلر، امرأة ذات شعر بني قصير وعيون خضراء، ترتدي سترة زرقاء”

**وصف ضعيف:**  
“فن خيالي”

**وصف أفضل:**  
“لوحة رقمية بأسلوب خيالي متوهج، تحتوي على فطر مضيء في غابة مظلمة، بخطوط تفصيلية ولوحة ألوان أرجوانية وزرقاء زاهية”

عبارة التفعيل (Trigger Phrase) التي تنوي استخدامها لاحقًا يجب أن تظهر في كل وصف. إذا كنت ستستخدم “بأسلوب luminescent fantasy” عند التوليد، فيجب أن تتكرر العبارة نفسها حرفيًا في جميع الملفات النصية.

يمكن كتابة الأوصاف يدويًا للمجموعات الصغيرة. أما المجموعات الكبيرة فيمكن استخدام أدوات مثل BLIP أو WD14 Tagger لتوليد أوصاف أولية، ثم مراجعتها وتعديلها يدويًا.

![هيكل مجلد منظم يحتوي على صور التدريب وملفات الوصف النصي المقابلة لها لتدريب LoRA](../_images/file-folder-organization.png)

### هيكل المجلدات

يجب تنظيم البيانات وفق بنية يتوقعها سكربت التدريب:

```

training_data/
├── 10_concept_name/
│   ├── image001.jpg
│   ├── image001.txt
│   ├── image002.jpg
│   ├── image002.txt
│   └── ...

```

الرقم في بداية اسم المجلد (مثل 10) يشير إلى عدد مرات تكرار كل صورة أثناء التدريب. الأرقام الأعلى تعطي وزنًا أكبر لتلك الصور في عملية التعلم.

الجزء الذي يلي الرقم والمفصول بشرطة سفلية يصبح عبارة التفعيل الافتراضية إذا لم تستخدم أوصافًا مخصصة.

---

## إعداد بيئة التدريب

بعد تجهيز البيانات واستئجار وحدة GPU، تأتي مرحلة إعداد بيئة التدريب. الأداة القياسية لتدريب LoRA هي kohya_ss/sd-scripts، وهي مجموعة سكربتات مفتوحة المصدر مدعومة من المجتمع.

### الإعداد الأولي

بعد الاتصال بالخادم، نفذ الأوامر التالية:

```bash
# استنساخ المستودع
git clone https://github.com/kohya-ss/sd-scripts.git
cd sd-scripts

# إنشاء بيئة افتراضية
python -m venv venv
source venv/bin/activate

# تثبيت الاعتمادات
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
pip install xformers
```

تستغرق هذه الخطوات عادةً بين خمس وعشر دقائق حسب سرعة الشبكة. تثبيت xformers اختياري لكنه موصى به، إذ يقلل استهلاك الذاكرة أثناء التدريب.

### تنزيل النموذج الأساسي

يجب تنزيل نموذج Stable Diffusion الأساسي:

```bash
mkdir -p models/sd

wget -O models/sd/v1-5-pruned.safetensors \
  "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors"
```

لتدريب SDXL، استخدم النموذج الأساسي الخاص به (حجمه يقارب 6.5 جيجابايت).

### رفع بيانات التدريب

يمكن نقل البيانات عبر SCP:

```bash
scp -r ./training_data user@gpu-instance-ip:~/sd-scripts/
```

أو تنزيلها مباشرة من تخزين سحابي.

### إعداد خاص بـ GPUFlow

عند استخدام GPUFlow، غالبًا ما تكون البيئة مُعدة مسبقًا:

```bash
cd /workspace/sd-scripts
```

هذا يوفر عادةً 15 إلى 20 دقيقة من وقت الإعداد، وهو عامل مهم عند جلسات التدريب القصيرة.

## ضبط معايير التدريب

تؤثر إعدادات التدريب بشكل مباشر على جودة النتائج ومدة التنفيذ. القيم التالية تمثل نقطة انطلاق متوازنة تعطي نتائج مستقرة دون استهلاك مفرط للموارد.

### المعايير الأساسية

أنشئ ملف إعداد باسم `training_config.toml`:

```toml
[model]
pretrained_model_name_or_path = "./models/sd/v1-5-pruned.safetensors"
v2 = false
v_parameterization = false

[dataset]
train_data_dir = "./training_data"
resolution = 512
batch_size = 2
enable_bucket = true
min_bucket_reso = 256
max_bucket_reso = 1024

[training]
output_dir = "./output"
output_name = "my_lora"
max_train_epochs = 10
learning_rate = 1e-4
unet_lr = 1e-4
text_encoder_lr = 5e-5
lr_scheduler = "cosine_with_restarts"
lr_warmup_steps = 100
network_dim = 32
network_alpha = 16
optimizer_type = "AdamW8bit"
mixed_precision = "fp16"
save_every_n_epochs = 2
save_model_as = "safetensors"
```

### شرح المعايير

**resolution**
يجب أن تتطابق مع دقة الاستخدام النهائي. استخدم 512 لـ SD 1.5 و1024 لـ SDXL.

**batch_size**
كلما زاد حجم الدفعة زادت سرعة التدريب، لكنه يتطلب VRAM أكبر. ابدأ بقيمة 2، وزدها إذا سمحت الذاكرة.

**max_train_epochs**
كل عصر يعني أن النموذج يرى جميع الصور مرة واحدة. عشرة عصور نقطة بداية مناسبة.

**learning_rate**
يحدد شدة التحديثات. القيم أعلاه محافظة. إن بدت النتائج ضعيفة، جرّب 2e-4 أو 3e-4.

**network_dim و network_alpha**
تحددان سعة LoRA. قيمة 32 مع alpha 16 توازن بين الجودة وحجم الملف. القيم الأعلى (64 أو 128) قد تلتقط تفاصيل أدق، لكنها تزيد خطر الإفراط في التخصيص (overfitting).

**optimizer_type**
AdamW8bit يقلل استهلاك الذاكرة بشكل ملحوظ دون تأثير ملموس على الجودة.

**mixed_precision**
استخدام FP16 يخفض استهلاك الذاكرة إلى النصف تقريبًا مقارنة بـ FP32.

### التعديل حسب نوع البطاقة

- RTX 4090 (24GB):
  batch_size = 4 لـ SD 1.5
  batch_size = 2 لـ SDXL

- RTX 3090 (24GB):
  batch_size = 2 لـ SD 1.5
  batch_size = 1 لـ SDXL

- A100 (40GB):
  batch_size = 6–8 لـ SD 1.5
  batch_size = 4 لـ SDXL

زيادة حجم الدفعة تقلل عدد خطوات التحسين المطلوبة تقريبًا بنفس النسبة.

![محرر أكواد يعرض ملف إعدادات تدريب LoRA مع معايير معدل التعلم وحجم الدفعة وأبعاد الشبكة](../_images/terminal-screenshot-code-editor.png)

---

## تنفيذ جلسة التدريب

لتشغيل التدريب:

```bash
accelerate launch --num_cpu_threads_per_process=4 train_network.py \
  --config_file="./training_config.toml" \
  --logging_dir="./logs"
```

### متابعة التقدم

سيظهر إخراج مشابه لما يلي:

```
epoch 1/10, step 50/500, loss=0.0823
epoch 1/10, step 100/500, loss=0.0756
```

يُفترض أن تنخفض قيمة loss خلال العصور الأولى ثم تستقر تدريجيًا. إذا بدأت بالارتفاع بعد انخفاض أولي، فقد يكون هناك إفراط في التخصيص. وإذا بقيت ثابتة، فقد يكون معدل التعلم منخفضًا.

### الحفظ المرحلي

يتم حفظ نقطة تحقق كل عصرين. يفيد ذلك في:

1. استئناف التدريب عند الانقطاع
2. مقارنة النتائج بين عصور مختلفة واختيار الأفضل

### أزمنة التدريب المتوقعة

لمجموعة مكونة من 50 صورة (SD 1.5):

| GPU      | الزمن التقريبي |
| -------- | -------------- |
| RTX 3090 | 90–120 دقيقة   |
| RTX 4090 | 60–90 دقيقة    |
| A100     | 45–60 دقيقة    |

SDXL يستغرق عادة 1.5 إلى 2 ضعف هذه المدة.

---

## التحقق من النموذج واختباره

عند اكتمال التدريب، ستحصل على ملف `.safetensors` داخل مجلد الإخراج.

### اختبار أولي

انقل الملف إلى بيئة Stable Diffusion التي تستخدمها، وضعه في مجلد LoRA المناسب.

### منهجية الاختبار

اختبر ما يلي:

- قوة LoRA عند 0.5 و0.7 و0.8 و1.0
- موقع عبارة التفعيل داخل النص
- استخدام بذور مختلفة (على الأقل خمس بذور)
- إدراج أو حذف المفهوم في الـ negative prompt

### تقييم الجودة

قيّم النتائج وفق:

**دقة المفهوم**
هل يعكس الناتج المفهوم الذي دربته؟

**الاندماج**
هل يمكن دمجه في مشاهد مختلفة بسلاسة؟

**العيوب المتكررة**
هل تظهر أنماط غير طبيعية أو تشوهات ثابتة؟

**المرونة**
هل يتكيف مع أوضاع وسياقات مختلفة؟

![مقارنة جانبية لصور مولدة بقيم قوة LoRA مختلفة توضح فروقات الجودة](../_images/side-by-side-comparison.png)

---

## استراتيجيات تحسين التكلفة

### التحضير المسبق

أنهِ قص الصور وكتابة الأوصاف وتنظيم الملفات قبل بدء الاستئجار.

### التدريب المتعدد في جلسة واحدة

تدريب ثلاثة نماذج في جلسة واحدة يوفر وقت الإعداد المتكرر.

### اختبار مبكر

توقف عند عصر 6 واختبر النتائج. إن كانت جيدة، لا داعي للاستمرار.

### الإنهاء الفوري

أوقف الخادم فور تنزيل ملفاتك. تركه يعمل لليلة واحدة قد يضاعف التكلفة.

### اختيار التوقيت

الأسعار تتغير حسب الطلب. الفترات خارج أوقات الذروة غالبًا أرخص.

---

## المشكلات الشائعة والحلول

### نفاد ذاكرة CUDA

- خفّض batch_size
- فعّل gradient checkpointing
- استخدم بطاقة بسعة أكبر

### عدم انخفاض loss

- زد learning_rate
- راجع الأوصاف النصية
- تحقق من صحة المسارات

### LoRA بلا تأثير

- تحقق من عبارة التفعيل
- زد الوزن أثناء التوليد
- جرّب نقطة تحقق مختلفة

### إفراط في التخصيص

- قلل عدد العصور
- خفّض network_dim
- زد تنوع البيانات

### بطء التدريب

- تحقق من استخدام GPU عبر nvidia-smi
- تأكد من تثبيت xformers
- فعّل mixed_precision

---

## الأسئلة الشائعة

### هل يمكنني استخدام LoRA المدرب تجاريًا؟

يعتمد ذلك على ترخيص النموذج الأساسي. Stable Diffusion 1.5 يخضع لترخيص CreativeML Open RAIL-M الذي يسمح بالاستخدام التجاري بشروط معينة. SDXL يملك ترخيصًا مشابهًا. كما يجب التأكد من أن صور التدريب لا تنتهك حقوقًا قانونية.

(بقية الأسئلة مذكورة في بيانات الصفحة أعلاه.)

---

## الخلاصة

لم يعد تدريب نماذج LoRA عملية مكلفة أو معقدة. مع التخطيط السليم، يمكن تنفيذ التدريب بتكلفة محدودة ونتائج عملية.

العامل الحاسم ليس قوة العتاد بل جودة البيانات ودقة الإعداد. ابدأ بمجموعة صغيرة، استخدم إعدادات محافظة، وقيّم النتائج بعناية. انخفاض تكلفة المحاولة يجعل التكرار جزءًا طبيعيًا من عملية التحسين.

للمقارنة بين مزودي تأجير وحدات GPU بمختلف الفئات السعرية، راجع:
[مقارنة أسعار تأجير وحدات GPU لعام 2026](/en/gpu-rental-pricing-comparison-2026/)

---

_تم تحديث هذا الدليل في 12 فبراير 2026. تختلف أسعار التأجير وإعدادات الأدوات بمرور الوقت. يُنصح بالتحقق من الأسعار الحالية قبل بدء أي مشروع تدريب._

```

---
```
